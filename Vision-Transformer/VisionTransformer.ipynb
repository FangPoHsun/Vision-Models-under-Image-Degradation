{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ra1ph2/Vision-Transformer/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7v2dPqfZFIV"
   },
   "source": [
    "#### Libraries Imported and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3bQIxfD9Ep0y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YS7R221iUMI0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:  NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU: ', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFc5EfTgZIBk"
   },
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_75W0devEPY7"
   },
   "source": [
    "##### Vision Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "idmPOFlDIGDZ"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention Module used to perform self-attention operation allowing the model to attend\n",
    "    information from different representation subspaces on an input sequence of embeddings.\n",
    "    The sequence of operations is as follows :-\n",
    "\n",
    "    Input -> Query, Key, Value -> ReshapeHeads -> Query.TransposedKey -> Softmax -> Dropout\n",
    "    -> AttentionScores.Value -> ReshapeHeadsBack -> Output\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Dimension size of the hidden embedding\n",
    "        heads: Number of parallel attention heads (Default=8)\n",
    "        activation: Optional activation function to be applied to the input while\n",
    "                    transforming to query, key and value matrixes (Default=None)\n",
    "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
    "\n",
    "    Methods:\n",
    "        _reshape_heads(inp) :- \n",
    "        Changes the input sequence embeddings to reduced dimension according to the number\n",
    "        of attention heads to parallelize attention operation\n",
    "        (batch_size, seq_len, embed_dim) -> (batch_size * heads, seq_len, reduced_dim)\n",
    "\n",
    "        _reshape_heads_back(inp) :-\n",
    "        Changes the reduced dimension due to parallel attention heads back to the original\n",
    "        embedding size\n",
    "        (batch_size * heads, seq_len, reduced_dim) -> (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        forward(inp) :-\n",
    "        Performs the self-attention operation on the input sequence embedding.\n",
    "        Returns the output of self-attention as well as atttention scores\n",
    "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim), (batch_size * heads, seq_len, seq_len)\n",
    "\n",
    "    Examples:\n",
    "        >>> attention = Attention(embed_dim, heads, activation, dropout)\n",
    "        >>> out, weights = attention(inp)\n",
    "    '''\n",
    "    def __init__(self, embed_dim, heads=8, activation=None, dropout=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.heads = heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = inp.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "\n",
    "        query = self.activation(self.query(inp))\n",
    "        key   = self.activation(self.key(inp))\n",
    "        value = self.activation(self.value(inp))\n",
    "\n",
    "        # output of _reshape_heads(): (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
    "        query = self._reshape_heads(query)\n",
    "        key   = self._reshape_heads(key)\n",
    "        value = self._reshape_heads(value)\n",
    "\n",
    "        # attention_scores: (batch_size * heads, seq_len, seq_len) | Softmaxed along the last dimension\n",
    "        attention_scores = self.softmax(torch.matmul(query, key.transpose(1, 2)))\n",
    "\n",
    "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
    "        out = torch.matmul(self.dropout(attention_scores), value)\n",
    "        \n",
    "        # output of _reshape_heads_back(): (batch_size, seq_len, embed_size)\n",
    "        out = self._reshape_heads_back(out)\n",
    "\n",
    "        return out, attention_scores\n",
    "\n",
    "    def _reshape_heads(self, inp):\n",
    "        # inp: (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = inp.size()\n",
    "\n",
    "        reduced_dim = self.embed_dim // self.heads\n",
    "        assert reduced_dim * self.heads == self.embed_dim\n",
    "        out = inp.reshape(batch_size, seq_len, self.heads, reduced_dim)\n",
    "        out = out.permute(0, 2, 1, 3)\n",
    "        out = out.reshape(-1, seq_len, reduced_dim)\n",
    "\n",
    "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
    "        return out\n",
    "\n",
    "    def _reshape_heads_back(self, inp):\n",
    "        # inp: (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
    "        batch_size_mul_heads, seq_len, reduced_dim = inp.size()\n",
    "        batch_size = batch_size_mul_heads // self.heads\n",
    "\n",
    "        out = inp.reshape(batch_size, self.heads, seq_len, reduced_dim)\n",
    "        out = out.permute(0, 2, 1, 3)\n",
    "        out = out.reshape(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        # out: (batch_size, seq_len, embed_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XYemII1gElcK"
   },
   "outputs": [],
   "source": [
    "# Check if Dropout should be used after second Linear Layer\n",
    "class FeedForward(nn.Module):\n",
    "    '''\n",
    "    FeedForward Network with two sequential linear layers with GELU activation function\n",
    "    ,applied to the output of self attention operation. The sequence of operations is as\n",
    "    follows :-\n",
    "    \n",
    "    Input -> FC1 -> GELU -> Dropout -> FC2 -> Output\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Dimension size of the hidden embedding\n",
    "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
    "                           and then scaled back to capture richer information (Default=1)\n",
    "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
    "\n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    Examples:\n",
    "        >>> FF = FeedForward(8, 1)\n",
    "        >>> out = FF(inp)\n",
    "    '''\n",
    "    def __init__(self, embed_dim, forward_expansion=1, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim * forward_expansion)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear(embed_dim * forward_expansion, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = inp.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "\n",
    "        out = self.dropout(self.activation(self.fc1(inp)))\n",
    "        # out = self.dropout(self.fc2(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        # out: (batch_size, seq_len, embed_dim)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uMrCi7DNGvCc"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    '''\n",
    "    Transformer Block combines both the attention module and the feed forward module with layer\n",
    "    normalization, dropout and residual connections. The sequence of operations is as follows :-\n",
    "    \n",
    "    Input -> LayerNorm1 -> Attention -> Residual -> LayerNorm2 -> FeedForward -> Output\n",
    "      |                                   |  |                                      |\n",
    "      |-------------Addition--------------|  |---------------Addition---------------|\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Dimension size of the hidden embedding\n",
    "        heads: Number of parallel attention heads (Default=8)\n",
    "        activation: Optional activation function to be applied to the input while\n",
    "                    transforming to query, key and value matrixes (Default=None)\n",
    "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
    "                           and then scaled back to capture richer information (Default=1)\n",
    "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
    "    \n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    Examples:\n",
    "        >>> TB = TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout)\n",
    "        >>> out = TB(inp)\n",
    "    '''\n",
    "    def __init__(self, embed_dim, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = Attention(embed_dim, heads, activation, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = FeedForward(embed_dim, forward_expansion, dropout)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, seq_len, embed_dim)\n",
    "        batch_size, seq_len, embed_dim = inp.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "\n",
    "        res = inp\n",
    "        out = self.norm1(inp)\n",
    "        out, _ = self.attention(out)\n",
    "        out = out + res\n",
    "        \n",
    "        res = out\n",
    "        out = self.norm2(out)\n",
    "        out = self.feed_forward(out)\n",
    "        out = out + res\n",
    "\n",
    "        # out: (batch_size, seq_len, embed_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kcBcReoPJ4NC"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    '''\n",
    "    Transformer combines multiple layers of Transformer Blocks in a sequential manner. The sequence\n",
    "    of the operations is as follows -\n",
    "\n",
    "    Input -> TB1 -> TB2 -> .......... -> TBn (n being the number of layers) -> Output\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Dimension size of the hidden embedding\n",
    "        layers: Number of Transformer Blocks in the Transformer\n",
    "        heads: Number of parallel attention heads (Default=8)\n",
    "        activation: Optional activation function to be applied to the input while\n",
    "                    transforming to query, key and value matrixes (Default=None)\n",
    "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
    "                           and then scaled back to capture richer information (Default=1)\n",
    "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
    "    \n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    Examples:\n",
    "        >>> transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
    "        >>> out = transformer(inp)\n",
    "    '''\n",
    "    def __init__(self, embed_dim, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.trans_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout) for i in range(layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        out = inp\n",
    "        for block in self.trans_blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        # out: (batch_size, seq_len, embed_dim)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XRN2k5-kLWYG"
   },
   "outputs": [],
   "source": [
    "# Not Exactly Same as Paper\n",
    "class ClassificationHead(nn.Module):\n",
    "    '''\n",
    "    Classification Head attached to the first sequence token which is used as the arbitrary \n",
    "    classification token and used to optimize the transformer model by applying Cross-Entropy \n",
    "    loss. The sequence of operations is as follows :-\n",
    "\n",
    "    Input -> FC1 -> GELU -> Dropout -> FC2 -> Output\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Dimension size of the hidden embedding\n",
    "        classes: Number of classification classes in the dataset\n",
    "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
    "\n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, embed_dim) -> (batch_size, classes)\n",
    "\n",
    "    Examples:\n",
    "        >>> CH = ClassificationHead(embed_dim, classes, dropout)\n",
    "        >>> out = CH(inp)\n",
    "    '''\n",
    "    def __init__(self, embed_dim, classes, dropout=0.1):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.classes = classes\n",
    "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2)\n",
    "        self.activation = nn.GELU()\n",
    "        self.fc2 = nn.Linear(embed_dim // 2, classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, embed_dim)\n",
    "        batch_size, embed_dim = inp.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "\n",
    "        out = self.dropout(self.activation(self.fc1(inp)))\n",
    "        # out = self.softmax(self.fc2(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        # out: (batch_size, classes) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M-8nXI7gPxIs"
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    '''\n",
    "    Vision Transformer is the complete end to end model architecture which combines all the above modules\n",
    "    in a sequential manner. The sequence of the operations is as follows -\n",
    "\n",
    "    Input -> CreatePatches -> ClassToken, PatchToEmbed , PositionEmbed -> Transformer -> ClassificationHead -> Output\n",
    "                                   |            | |                |\n",
    "                                   |---Concat---| |----Addition----|\n",
    "    \n",
    "    Args:\n",
    "        patch_size: Length of square patch size \n",
    "        max_len: Max length of learnable positional embedding\n",
    "        embed_dim: Dimension size of the hidden embedding\n",
    "        classes: Number of classes in the dataset\n",
    "        layers: Number of Transformer Blocks in the Transformer\n",
    "        channels: Number of channels in the input (Default=3)\n",
    "        heads: Number of parallel attention heads (Default=8)\n",
    "        activation: Optional activation function to be applied to the input while\n",
    "                    transforming to query, key and value matrixes (Default=None)\n",
    "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
    "                           and then scaled back to capture richer information (Default=1)\n",
    "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
    "    \n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        It outputs the classification output as well as the sequence output of the transformer\n",
    "        (batch_size, channels, width, height) -> (batch_size, classes), (batch_size, seq_len+1, embed_dim)\n",
    "    \n",
    "    Examples:\n",
    "        >>> ViT = VisionTransformer(atch_size, max_len, embed_dim, classes, layers, channels, heads, activation, forward_expansion, dropout)\n",
    "        >>> class_out, hidden_seq = ViT(inp)\n",
    "    '''\n",
    "    def __init__(self, patch_size, max_len, embed_dim, classes, layers, channels=3, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.name = 'VisionTransformer'\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.channels = channels\n",
    "        self.patch_to_embed = nn.Linear(patch_size * patch_size * channels, embed_dim)\n",
    "        self.position_embed = nn.Parameter(torch.randn((max_len, embed_dim)))\n",
    "        self.transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
    "        self.classification_head = ClassificationHead(embed_dim, classes)\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, channels, width, height)\n",
    "        batch_size, channels, width, height = inp.size()\n",
    "        assert channels == self.channels\n",
    "\n",
    "        out = inp.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size).contiguous()\n",
    "        out = out.view(batch_size, channels, -1, self.patch_size, self.patch_size)\n",
    "        out = out.permute(0, 2, 3, 4, 1)\n",
    "        # out: (batch_size, seq_len, patch_size, patch_size, channels) | seq_len would be (width*height)/(patch_size**2)\n",
    "        batch_size, seq_len, patch_size, _, channels = out.size()\n",
    "        \n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "        out = self.patch_to_embed(out)\n",
    "        # out: (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        out = torch.cat([class_token, out], dim=1)\n",
    "        # out: (batch_size, seq_len+1, embed_dim)\n",
    "\n",
    "        position_embed = self.position_embed[:seq_len+1]\n",
    "        position_embed = position_embed.unsqueeze(0).expand(batch_size, seq_len+1, self.embed_dim)\n",
    "        out = out + position_embed\n",
    "        # out: (batch_size, seq_len+1, embed_dim) | Added Positional Embeddings\n",
    "\n",
    "        out = self.transformer(out)\n",
    "        # out: (batch_size, seq_len+1, embed_dim) \n",
    "        class_token = out[:, 0]\n",
    "        # class_token: (batch_size, embed_dim)\n",
    "\n",
    "        class_out = self.classification_head(class_token)\n",
    "        # class_out: (batch_size, classes)\n",
    "        \n",
    "        return class_out, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hJPR42t1S9li"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "class ResNetFeatures(nn.Module):\n",
    "    '''\n",
    "    ResNetFeatures outputs the lower level features from pretrained ResNet34 till the intial 5 layers \n",
    "    (conv1, bn1, relu, maxpool, layer1(3 conv layers)) to be used in the hybrid architecture to be \n",
    "    able to kickstart the learining faster. The sequence of operations is as follows :-\n",
    "\n",
    "    Input -> conv1 -> bn1 -> relu -> maxpool -> layer1 -> Output\n",
    "\n",
    "    Args:\n",
    "        No arguments required\n",
    "    \n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, 3, 224, 224) -> (batch_size, 64, 56, 56)\n",
    "    \n",
    "    Examples:\n",
    "        >>> resnet_features = ResNetFeatures()\n",
    "        >>> out = resnet_features(inp)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ResNetFeatures, self).__init__()\n",
    "        layers = list(resnet34(pretrained=True).children())[:5] #all layer expect last layer\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, 3, 224, 224)\n",
    "\n",
    "        out = self.feature_extractor(inp)\n",
    "\n",
    "        # out: (batch_size, 64, 56, 56)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsEV_yqAETZA"
   },
   "source": [
    "##### ResNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eUMIU5BMjmeD"
   },
   "outputs": [],
   "source": [
    "class ResidualBlockSmall(nn.Module):\n",
    "    '''\n",
    "    ResidualBlockSmall implements the smaller block of the Residual Networks. It optionally also downsamples\n",
    "    the input according to the stride to match the output while adding the residual. The sequence of operations\n",
    "    is as follows :-\n",
    "\n",
    "    Input -> Conv1 -> BNorm1 -> ReLU -> Conv2 -> BNorm2 -> ReLU -> Output\n",
    "      |                                                              |\n",
    "      |-----------------Residual_Downsample (Optional)---------------|\n",
    "\n",
    "    Args:\n",
    "        input_channels: Number of input channels\n",
    "        out_channels: Number of output channels\n",
    "        residual_downsample: Residual Downsample dependent on if either height, width or channels change\n",
    "        stride: Stride value for the convolutional layers (Default=1)\n",
    "\n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, input_channels, height, width) -> (batch_size, out_channels, height, width)\n",
    "    \n",
    "    Examples:\n",
    "        >>> RBS = ResidualBlockSmall(input_channels, out_channels, residual_downsample, stride)\n",
    "        >>> out = RBS(inp)\n",
    "    '''\n",
    "    def __init__(self, input_channels, out_channels, residual_downsample=None, stride=1):\n",
    "        super(ResidualBlockSmall, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.residual_downsample = residual_downsample\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, input_channels, height, width)\n",
    "\n",
    "        res = inp\n",
    "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
    "        out = self.activation(self.bnorm2(self.conv2(out)))\n",
    "        \n",
    "        if self.residual_downsample is not None:\n",
    "            res = self.residual_downsample(res)\n",
    "\n",
    "        out = self.activation(out + res)\n",
    "\n",
    "        # out: (batch_size, out_channels, height, width) | height, width depending on stride\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KvBRaEPVktZd"
   },
   "outputs": [],
   "source": [
    "class ResNetSmall(nn.Module):\n",
    "    '''\n",
    "    ResNetSmall consists of layers of the smaller residual block defined above (ResidualBlockSmall).\n",
    "    The layers are the residual blocks. The sequence of operations is as follows :-\n",
    "\n",
    "    Input -> Conv1 -> BNorm1 -> ReLU -> MaxPool -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> AvgPool -> FC\n",
    "\n",
    "    Args:\n",
    "        layers: A four value array containing number of conv layers in each residual block\n",
    "        input_channels: number of input channels\n",
    "        classes: Number of classes in the dataset\n",
    "    \n",
    "    Methods:\n",
    "        _layer(num_layers (Number of conv layers)\n",
    "               ,input_channels (Number of input channels)\n",
    "               ,output_channels (Number of output channels)\n",
    "               ,stride (Stride value for conv layer)) :-\n",
    "        Returns the sequential wrapper with all the layers in the residual block constructed according\n",
    "        to the parameters.\n",
    "\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, input_channels, height, width) -> (batch_size, classes)\n",
    "\n",
    "    Examples:\n",
    "        >>> resnet = ResNetSmall(layers, input_channels, classes)\n",
    "        >>> out = resnet(inp)\n",
    "    '''\n",
    "    def __init__(self, layers, input_channels, classes):\n",
    "        super(ResNetSmall, self).__init__()\n",
    "        self.name = 'ResNet'\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bnorm1 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._layer(layers[0], input_channels=64, output_channels=64, stride=1)\n",
    "        self.layer2 = self._layer(layers[1], input_channels=64, output_channels=128, stride=2)\n",
    "        self.layer3 = self._layer(layers[2], input_channels=128, output_channels=256, stride=2)\n",
    "        self.layer4 = self._layer(layers[3], input_channels=256, output_channels=512, stride=2)\n",
    "\n",
    "        self.avppool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, classes)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, input_channels, height, width)\n",
    "\n",
    "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avppool(out)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # out: (batch_size, classes)\n",
    "        return out\n",
    "\n",
    "    def _layer(self, num_layers, input_channels, output_channels, stride):\n",
    "        residual_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        if stride != 1:\n",
    "            residual_downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(output_channels * 4)\n",
    "            )\n",
    "        \n",
    "        layers.append(ResidualBlockSmall(input_channels, output_channels, residual_downsample, stride))\n",
    "\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(ResidualBlockSmall(output_channels, output_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vQxpId2jEV_R"
   },
   "outputs": [],
   "source": [
    "class ResidualBlockLarge(nn.Module):\n",
    "    '''\n",
    "    ResidualBlockLarge implements the larger block of the Residual Networks. It optionally also downsamples\n",
    "    the input according to the stride or output channels to match the output while adding the residual. The \n",
    "    sequence of operations is as follows :-\n",
    "\n",
    "    Input -> Conv1 -> BNorm1 -> ReLU -> Conv2 -> BNorm2 -> ReLU -> Conv3 -> BNorm3 -> ReLU -> Output\n",
    "      |                                                                                          |\n",
    "      |-----------------------------Residual_Downsample (Optional)-------------------------------|\n",
    "\n",
    "    Args:\n",
    "        input_channels: Number of input channels\n",
    "        out_channels: Number of output channels\n",
    "        residual_downsample: Residual Downsample dependent on if either height, width or channels change\n",
    "        stride: Stride value for the convolutional layers (Default=1)\n",
    "        expansion: Expansion of the input channels during convolutions (Default=4)\n",
    "\n",
    "    Methods:\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, input_channels, height, width) -> (batch_size, out_channels * expansion, height, width)\n",
    "    \n",
    "    Examples:\n",
    "        >>> RBL = ResidualBlockLarge(input_channels, out_channels, residual_downsample, stride, expansion)\n",
    "        >>> out = RBL(inp)\n",
    "    '''\n",
    "    def __init__(self, input_channels, out_channels, residual_downsample=None, stride=1, expansion=4):\n",
    "        super(ResidualBlockLarge, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * expansion, kernel_size=1, stride=1, padding=0)\n",
    "        self.bnorm3 = nn.BatchNorm2d(out_channels * expansion)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.residual_downsample = residual_downsample\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, input_channels, height, width)\n",
    "\n",
    "        res = inp\n",
    "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
    "        out = self.activation(self.bnorm2(self.conv2(out)))\n",
    "        out = self.activation(self.bnorm3(self.conv3(out)))\n",
    "        \n",
    "        if self.residual_downsample is not None:\n",
    "            res = self.residual_downsample(res)\n",
    "\n",
    "        out = self.activation(out + res)\n",
    "\n",
    "        # out: (batch_size, out_channels * expansion, height, width) | height, width depending on stride\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6ss--kZkSGNU"
   },
   "outputs": [],
   "source": [
    "class ResNetLarge(nn.Module):\n",
    "    '''\n",
    "    ResNetLarge consists of layers of the larger residual block defined above (ResidualBlockLarger).\n",
    "    The layers are the residual blocks. The sequence of operations is as follows :-\n",
    "\n",
    "    Input -> Conv1 -> BNorm1 -> ReLU -> MaxPool -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> AvgPool -> FC\n",
    "\n",
    "    Args:\n",
    "        layers: A four value array containing number of conv layers in each residual block\n",
    "        input_channels: number of input channels\n",
    "        classes: Number of classes in the dataset\n",
    "    \n",
    "    Methods:\n",
    "        _layer(num_layers (Number of conv layers)\n",
    "               ,input_channels (Number of input channels)\n",
    "               ,output_channels (Number of output channels)\n",
    "               ,stride (Stride value for conv layer)) :-\n",
    "        Returns the sequential wrapper with all the layers in the residual block constructed according\n",
    "        to the parameters.\n",
    "\n",
    "        forward(inp) :-\n",
    "        Applies the sequence of operations mentioned above.\n",
    "        (batch_size, input_channels, height, width) -> (batch_size, classes)\n",
    "\n",
    "    Examples:\n",
    "        >>> resnet = ResNetLarge(layers, input_channels, classes)\n",
    "        >>> out = resnet(inp)\n",
    "    '''\n",
    "    def __init__(self, layers, input_channels, classes):\n",
    "        super(ResNetLarge, self).__init__()\n",
    "        self.name = 'ResNet'\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bnorm1 = nn.BatchNorm2d(64)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._layer(layers[0], input_channels=64, output_channels=64, stride=1)\n",
    "        self.layer2 = self._layer(layers[1], input_channels=256, output_channels=128, stride=2)\n",
    "        self.layer3 = self._layer(layers[2], input_channels=512, output_channels=256, stride=2)\n",
    "        self.layer4 = self._layer(layers[3], input_channels=1024, output_channels=512, stride=2)\n",
    "\n",
    "        self.avppool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, classes)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp: (batch_size, input_channels, height, width)\n",
    "\n",
    "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avppool(out)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # out: (batch_size, classes)\n",
    "        return out\n",
    "\n",
    "    def _layer(self, num_layers, input_channels, output_channels, stride):\n",
    "        residual_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Checks if there would be potential mismatch in any of height, width or channels between input and output. \n",
    "        # 4 is the value of the expansion for large ResNets\n",
    "        if stride != 1 or input_channels != output_channels * 4:\n",
    "            residual_downsample = nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels * 4, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(output_channels * 4)\n",
    "            )\n",
    "        \n",
    "        layers.append(ResidualBlockLarge(input_channels, output_channels, residual_downsample, stride))\n",
    "\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(ResidualBlockLarge(output_channels * 4, output_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4piMb4xwl3MX"
   },
   "outputs": [],
   "source": [
    "def ResNet34(input_channels, classes):\n",
    "    '''\n",
    "    Initalization of ResNet34 using the layers as mentioned in the paper and using ResNetSmall module.\n",
    "\n",
    "    Args:\n",
    "        input_channels: Number of input channels\n",
    "        classes: Number of classes in the dataset\n",
    "    \n",
    "    Output:\n",
    "        ResNetSmall Object\n",
    "    '''\n",
    "    return ResNetSmall([3, 4, 6, 3], input_channels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GeceF_RUaVO4"
   },
   "outputs": [],
   "source": [
    "def ResNet50(input_channels, classes):\n",
    "    '''\n",
    "    Initalization of ResNet50 using the layers as mentioned in the paper and using ResNetLarge module.\n",
    "    \n",
    "    Args:\n",
    "        input_channels: Number of input channels\n",
    "        classes: Number of classes in the dataset\n",
    "    \n",
    "    Output:\n",
    "        ResNetLarge Object\n",
    "    '''\n",
    "    return ResNetLarge([3, 4, 6, 3], input_channels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGS4bmCRWmIg"
   },
   "source": [
    "#### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "60fXexWWWlpJ"
   },
   "outputs": [],
   "source": [
    "def CIFAR100DataLoader(split, batch_size=8, num_workers=2, shuffle=True, size='32', normalize='standard'):\n",
    "    '''\n",
    "    A wrapper function that creates a DataLoader for CIFAR100 dataset loaded from torchvision using \n",
    "    the parameters supplied and applies the required data augmentations.\n",
    "\n",
    "    Args:\n",
    "        split: A string to decide if train or test data to be used (Values: 'train', 'test')\n",
    "        batch_size: Batch size to used for loading data (Default=8)\n",
    "        num_workers: Number of parallel workers used to load data (Default=2)\n",
    "        shuffle: Boolean value to decide if data should be randomized (Default=True)\n",
    "        size: A string to decide the size of the input images (Default='32') (Values: '32','224')\n",
    "        normalize: A string to decide the normalization to applied to the input images\n",
    "                   (Default='standard') (Values: 'standard', 'imagenet')\n",
    "    \n",
    "    Output:\n",
    "        DataLoader Object\n",
    "    '''\n",
    "    if normalize == 'imagenet':\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    elif normalize == 'standard':\n",
    "        mean = [0.5, 0.5, 0.5]\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "\n",
    "    if split == 'train':\n",
    "        if size == '224':\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop((224,224), scale=(0.8, 1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        elif size == '32':\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        \n",
    "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
    "    \n",
    "    elif split == 'test':\n",
    "        if size == '224':\n",
    "            test_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "\n",
    "        elif size == '32':\n",
    "            test_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "\n",
    "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
    "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RtYCa72d0RJ6"
   },
   "outputs": [],
   "source": [
    "def CIFAR10DataLoader(split, batch_size=8, num_workers=2, shuffle=True, size='32', normalize='standard'):\n",
    "    '''\n",
    "    A wrapper function that creates a DataLoader for CIFAR10 dataset loaded from torchvision using \n",
    "    the parameters supplied and applies the required data augmentations.\n",
    "\n",
    "    Args:\n",
    "        split: A string to decide if train or test data to be used (Values: 'train', 'test')\n",
    "        batch_size: Batch size to used for loading data (Default=8)\n",
    "        num_workers: Number of parallel workers used to load data (Default=2)\n",
    "        shuffle: Boolean value to decide if data should be randomized (Default=True)\n",
    "        size: A string to decide the size of the input images (Default='32') (Values: '32','224')\n",
    "        normalize: A string to decide the normalization to applied to the input images\n",
    "                   (Default='standard') (Values: 'standard', 'imagenet')\n",
    "    \n",
    "    Output:\n",
    "        DataLoader Object\n",
    "    '''\n",
    "    if normalize == 'imagenet':\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    elif normalize == 'standard':\n",
    "        mean = [0.5, 0.5, 0.5]\n",
    "        std =  [0.5, 0.5, 0.5]\n",
    "\n",
    "    if split == 'train':\n",
    "        if size == '224':\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop((224,224), scale=(0.5, 1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        elif size == '32':\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.Resize((48, 48)),\n",
    "                transforms.RandomCrop(32),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(15),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        \n",
    "        cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        dataloader = DataLoader(cifar10, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
    "    \n",
    "    elif split == 'test':\n",
    "        if size == '224':\n",
    "            test_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        elif size == '32':\n",
    "            test_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "\n",
    "        cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "        dataloader = DataLoader(cifar10, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UB_YjSG0aLJR"
   },
   "source": [
    "#### Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Byfz7zrEaKxq"
   },
   "outputs": [],
   "source": [
    "# Initializations of all the constants used in the training and testing process\n",
    "\n",
    "lr = 0.003\n",
    "batch_size = 256\n",
    "num_workers = 2\n",
    "shuffle = True\n",
    "patch_size = 4\n",
    "image_sz = 32\n",
    "max_len = 100 # All sequences must be less than 1000 including class token\n",
    "embed_dim = 512\n",
    "classes = 10\n",
    "layers = 12\n",
    "channels = 3\n",
    "resnet_features_channels = 64\n",
    "heads = 16\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d89pJGwZaUBs"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, scheduler, resnet_features=None):\n",
    "    '''\n",
    "    Function used to train the model over a single epoch and update it according to the\n",
    "    calculated gradients.\n",
    "\n",
    "    Args:\n",
    "        model: Model supplied to the function\n",
    "        dataloader: DataLoader supplied to the function\n",
    "        criterion: Criterion used to calculate loss\n",
    "        optimizer: Optimizer used update the model\n",
    "        scheduler: Scheduler used to update the learing rate for faster convergence \n",
    "                   (Commented out due to poor results)\n",
    "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
    "\n",
    "    Output:\n",
    "        running_loss: Training Loss (Float)\n",
    "        running_accuracy: Training Accuracy (Float)\n",
    "    '''\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for data, target in (dataloader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        if model.name == 'VisionTransformer':\n",
    "            with torch.no_grad():\n",
    "                if resnet_features != None:\n",
    "                    data = resnet_features(data)\n",
    "            output, _ = model(data)\n",
    "        elif model.name == 'ResNet':\n",
    "            output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == target).float().mean()\n",
    "        running_accuracy += acc / len(dataloader)\n",
    "        running_loss += loss.item() / len(dataloader)\n",
    "\n",
    "    return running_loss, running_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dXAFKy14EIyU"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, criterion, resnet_features=None):\n",
    "    '''\n",
    "    Function used to evaluate the model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Model supplied to the function\n",
    "        dataloader: DataLoader supplied to the function\n",
    "        criterion: Criterion used to calculate loss\n",
    "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
    "    \n",
    "    Output:\n",
    "        test_loss: Testing Loss (Float)\n",
    "        test_accuracy: Testing Accuracy (Float)\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        test_accuracy = 0.0\n",
    "        test_loss = 0.0\n",
    "        for data, target in (dataloader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            if model.name == 'VisionTransformer':\n",
    "                if resnet_features != None:\n",
    "                    data = resnet_features(data)\n",
    "                output, _ = model(data)\n",
    "            elif model.name == 'ResNet':\n",
    "                output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == target).float().mean()\n",
    "            test_accuracy += acc / len(dataloader)\n",
    "            test_loss += loss.item() / len(dataloader)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1NNc-FxvJU7"
   },
   "source": [
    "#### Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94FIkGknvMUW"
   },
   "source": [
    "Run either one the following subcells according to the models selected to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofAkCvMzfN3R"
   },
   "source": [
    "##### Model - Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVBkOJnnvWzH"
   },
   "source": [
    "Recommended Values for the following Architecture\n",
    "\n",
    "- patch_size = 4\n",
    "- max_len = 100\n",
    "- embed_dim = 512\n",
    "- classes = According to Dataset\n",
    "- layers = 12\n",
    "- channels = 3\n",
    "- heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WG_wT7pXe5q9"
   },
   "outputs": [],
   "source": [
    "# Vision Transformer Architecture\n",
    "\n",
    "model = VisionTransformer(\n",
    "    patch_size=patch_size,\n",
    "    max_len=max_len,\n",
    "    embed_dim=embed_dim,\n",
    "    classes=classes,\n",
    "    layers=layers,\n",
    "    channels=channels,\n",
    "    heads=heads).to(device)\n",
    "\n",
    "resnet_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AtT5cLKvtxC"
   },
   "source": [
    "Recommended Values for the following Architecture\n",
    "\n",
    "- patch_size = 7\n",
    "- max_len = 100\n",
    "- embed_dim = 512\n",
    "- classes = According to Dataset\n",
    "- layers = 12\n",
    "- channels = 64 (Resnet Features Channels)\n",
    "- heads = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MrWmtB8QVT50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Vision Transformer Architecture\n",
    "\n",
    "model = VisionTransformer(\n",
    "    patch_size=patch_size,\n",
    "    max_len=max_len,\n",
    "    embed_dim=embed_dim,\n",
    "    classes=classes,\n",
    "    layers=layers,\n",
    "    channels=resnet_features_channels,\n",
    "    heads=heads).to(device)\n",
    "\n",
    "resnet_features = ResNetFeatures().to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq_fd2gjfRqD"
   },
   "source": [
    "##### Model - ResNet50 or ResNet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MduGlJkPwEmp"
   },
   "source": [
    "Recommended Values for the following Architecture\n",
    "\n",
    "- input_channels = 3\n",
    "- classes = According to Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NJhrjCYhmjII"
   },
   "outputs": [],
   "source": [
    "# ResNet34 Architecture\n",
    "\n",
    "model = ResNet34(\n",
    "    input_channels=3,\n",
    "    classes=classes).to(device)\n",
    "\n",
    "resnet_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQzAinFRwMcw"
   },
   "source": [
    "Recommended Values for the following Architecture\n",
    "\n",
    "- input_channels = 3\n",
    "- classes = According to Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "nztkBOLRe6pO"
   },
   "outputs": [],
   "source": [
    "# ResNet50 Architecture\n",
    "\n",
    "model = ResNet50(\n",
    "    input_channels=3,\n",
    "    classes=classes).to(device)\n",
    "\n",
    "resnet_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0x9qE4HhUhR"
   },
   "source": [
    "#### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPVinX5pw46Q"
   },
   "source": [
    "##### CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vA4SFPwTwxcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f202be54cfe046a1856052b7a66ffdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1\n",
      "train acc :  0.3927 - train loss : 1.6541\n",
      "\n",
      "test acc : 0.4079 - test loss : 1.6487\n",
      "\n",
      "Epoch : 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 14\u001b[0m running_loss, running_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresnet_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain acc :  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - train loss : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m train_accs\u001b[38;5;241m.\u001b[39mappend(running_accuracy)\n",
      "Cell \u001b[1;32mIn[21], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, scheduler, resnet_features)\u001b[0m\n\u001b[0;32m     19\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     20\u001b[0m running_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m (dataloader):\n\u001b[0;32m     23\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\openilt\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataloader = CIFAR10DataLoader(split='train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, size='32', normalize='standard')\n",
    "test_dataloader = CIFAR10DataLoader(split='test', batch_size=batch_size, num_workers=num_workers, shuffle=False, size='32', normalize='standard')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch : {epoch+1}\")\n",
    "\n",
    "    model.train()\n",
    "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler, resnet_features)\n",
    "    print(f\"train acc :  {running_accuracy:.4f} - train loss : {running_loss:.4f}\\n\")\n",
    "    train_accs.append(running_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, test_accuracy = evaluation(model, test_dataloader, criterion, resnet_features)\n",
    "    print(f\"test acc : {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
    "    test_accs.append(test_accuracy)\n",
    "\n",
    "    if (epoch+ 1 ) % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'train_acc': train_accs,\n",
    "            'test_acc': test_accs\n",
    "        }, r'C:\\Users\\User\\OneDrive\\桌面\\Image Processing Final Project\\Vision-Transformer-main' + model.name + '_CIFAR10_checkpoint.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7yDEtyzw8oU"
   },
   "source": [
    "##### CIFAR100 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp1l68Z_UiGc"
   },
   "outputs": [],
   "source": [
    "train_dataloader = CIFAR100DataLoader(split='train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, size='32', normalize='standard')\n",
    "test_dataloader = CIFAR100DataLoader(split='test', batch_size=batch_size, num_workers=num_workers, shuffle=False, size='32', normalize='standard')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler, resnet_features)\n",
    "    print(f\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\")\n",
    "    train_accs.append(running_accuracy)\n",
    "\n",
    "    test_loss, test_accuracy = evaluation(model, test_dataloader, criterion, resnet_features)\n",
    "    print(f\"test acc: {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
    "    test_accs.append(test_accuracy)\n",
    "\n",
    "    if (epoch+1)%5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model': model,\n",
    "            'optimizer': optimizer,\n",
    "            'scheduler': scheduler,\n",
    "            'train_acc': train_accs,\n",
    "            'test_acc': test_accs\n",
    "        }, './drive/MyDrive/VisionTransformer/' + model.name + '_CIFAR100_checkpoint.pt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KjgBoZoxU42"
   },
   "source": [
    "##### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WsB1V8Gl0jnQ"
   },
   "outputs": [],
   "source": [
    "train_accs = [acc.cpu().item() for acc in train_accs]\n",
    "test_accs = [acc.cpu().item() for acc in test_accs]\n",
    "# print(train_accs)\n",
    "# print(test_accs)\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(range(1, 101), train_accs, label='Train Accuracy')\n",
    "plt.plot(range(1, 101), test_accs, label='Test Accuracy')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.title(\"Train vs Test Accuracy\")\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNCZTp0j+sS0hGspyPCbLz6",
   "include_colab_link": true,
   "mount_file_id": "194YTo4VqBt4HXEGcBz__l49llDopmNMP",
   "name": "VisionTransformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "openilt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
